---
layout: page
title: Organizers
permalink: /organizers/
---

<table style="margin-left: auto; margin-right: auto; width: 90%;border-collapse: collapse;">
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Xinyi Wang"></td>
    <td style="border: none;"><a href="mailto:xinyi_wang@ucsb.edu">Xinyi Wang</a><br>Postdoctoral researcher at Princeton University, and an incoming assistant professor at the University at Buffalo, SUNY. She recently defended her Ph.D. at the University of California, Santa Barbara (UCSB). She has received a J.P. Morgan AI Ph.D. Fellowship. Her research centers on developing a principled understanding of large foundation models, with the aim of enhancing their capabilities and addressing their limitations.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Nikunj Saunshi"></td>
    <td style="border: none;"><a href="mailto:nsaunshi@google.com">Nikunj Saunshi</a><br>Senior Research Scientist at Google. His research interests lie in interweaving theory and empirics to design efficient and reliable learning algorithms and to demystify deep learning and AI. His research has spanned topics like reasoning in large language models, self-supervised representation learning, meta-learning, NLP, interpretability of deep learning models. He received a PhD in Computer Science from Princeton University and was a recipient of the 2022 Siebel Scholars award.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Rui-Jie Zhu"></td>
    <td style="border: none;"><a href="mailto:ridger@ucsc.edu">Rui-Jie Zhu</a><br>Ph.D. student at UC Santa Cruz, advised by Professor Jason K. Eshraghian, specializing in efficient deep learning with a particular focus on Linear Attention mechanisms. He is a key contributor to the development of RWKV. His work also includes notable projects like SpikeGPT. He is a recipient of the UC Santa Cruz Chancellor's 2024 Innovation Impact Award. Rui-Jie's research is dedicated to addressing the computational bottlenecks of traditional attention mechanisms and advancing scalable, efficient AI systems.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Liu Yang"></td>
    <td style="border: none;"><a href="mailto:liu.yang@wisc.edu">Liu Yang</a><br>PhD student at the University of Wisconsinâ€“Madison, advised by Prof. Robert Nowak, Prof. Dimitris Papailiopoulos, and Prof. Kangwook Lee. Her research focuses on improving the efficiency of large language models by understanding and leveraging their internal representations, such as latent embeddings and task vectors. She has co-authored multiple publications in top-tier machine learning conferences, including NeurIPS, ICLR, and ICML.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Yuntian Deng"></td>
    <td style="border: none;"><a href="mailto:yuntian@uwaterloo.ca">Yuntian Deng</a><br>Assistant professor at the University of Waterloo. Previously, he was a postdoc at AI2, advised by Prof. Yejin Choi, and earned his PhD from Harvard University under Profs. Alexander Rush and Stuart Shieber. His research focuses on natural language processing and machine learning, particularly implicit chain-of-thought (CoT) reasoning. In 2023, he introduced the idea of implicit CoT reasoning using hidden states in Implicit Chain-of-Thought Reasoning via Knowledge Distillation, and further advanced this idea in From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Nishanth Dikkala"></td>
    <td style="border: none;"><a href="mailto:nishanthd@google.com">Nishanth Dikkala</a><br>Senior Research Scientist at Google. He is interested in efficient architecture design, augmenting LLM reasoning capabilities, and learning theory. He holds a Ph.D. from MIT and a Bachelors from Indian Institute of Technology Bombay.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Jiaheng Liu"></td>
    <td style="border: none;"><a href="mailto:liujiaheng@nju.edu.cn">Jiaheng Liu</a><br>Assistant professor at Nanjing University. His current research mainly focuses on Large Language Models (LLMs), with contributions in pre-training, alignment, and open science of LLMs. He has published 60+ top conference/journal papers, including the ACL 2024 Outstanding Paper Award. Additionally, he has served as the invited speaker of the first GLOW workshop at IJCAI 2023, and the organizer of the first SCI-FM workshop at ICLR 2025.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Zhiyuan Li"></td>
    <td style="border: none;"><a href="mailto:zhiyuanli@ttic.edu">Zhiyuan Li</a><br>Assistant professor in the Toyota Technological Institute at Chicago (TTIC) and an affiliated faculty at Uchicago CS. He is currently a visiting faculty at Google Research. He obtained his Ph.D. in computer science at Princeton University in 2022 and spent one year for postdoc at Stanford CS. His research focuses on machine learning theory. He is a recipient of Microsoft Research PhD Fellowship. He is the leading organizer of the first and second workshop for Mathematics in Modern Machine Learning (M3L) at NeurIPS 2023 and 2024.</td>
  </tr>
</table>
