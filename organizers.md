---
layout: page
title: Organizers
permalink: /organizers/
---

<table style="margin-left: auto; margin-right: auto; width: 90%;border-collapse: collapse;">
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Xinyi Wang"></td>
    <td style="border: none;"><a href="https://wangxinyilinda.github.io/">Xinyi Wang</a><br>Postdoctoral researcher at Princeton University, and an incoming assistant professor at the University at Buffalo, SUNY. She recently defended her Ph.D. at the University of California, Santa Barbara (UCSB). She has received a J.P. Morgan AI Ph.D. Fellowship. Her research centers on developing a principled understanding of large foundation models, with the aim of enhancing their capabilities and addressing their limitations.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Nikunj Saunshi"></td>
    <td style="border: none;"><a href="https://nikunjsaunshi.com/">Nikunj Saunshi</a><br>Senior Research Scientist at Google. His research interests lie in interweaving theory and empirics to design efficient and reliable learning algorithms and to demystify deep learning and AI. His research has spanned topics like reasoning in large language models, self-supervised representation learning, meta-learning, NLP, interpretability of deep learning models. He received a PhD in Computer Science from Princeton University and was a recipient of the 2022 Siebel Scholars award.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Rui-Jie Zhu"></td>
    <td style="border: none;"><a href="https://ruijie-zhu.github.io/">Rui-Jie Zhu</a><br>Ph.D. student at UC Santa Cruz, advised by Professor Jason K. Eshraghian, specializing in efficient deep learning with a particular focus on Linear Attention mechanisms. He is a key contributor to the development of RWKV. His work also includes notable projects like SpikeGPT. He is a recipient of the UC Santa Cruz Chancellor's 2024 Innovation Impact Award. Rui-Jie's research is dedicated to addressing the computational bottlenecks of traditional attention mechanisms and advancing scalable, efficient AI systems.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Liu Yang"></td>
    <td style="border: none;"><a href="https://leiay.github.io/">Liu Yang</a><br>PhD student at the University of Wisconsinâ€“Madison, advised by Prof. Robert Nowak, Prof. Dimitris Papailiopoulos, and Prof. Kangwook Lee. Her research focuses on improving the efficiency of large language models by understanding and leveraging their internal representations, such as latent embeddings and task vectors. She has co-authored multiple publications in top-tier machine learning conferences, including NeurIPS, ICLR, and ICML.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Yuntian Deng"></td>
    <td style="border: none;"><a href="https://yuntiandeng.com/">Yuntian Deng</a><br>Assistant professor at the University of Waterloo. Previously, he was a postdoc at AI2, advised by Prof. Yejin Choi, and earned his PhD from Harvard University under Profs. Alexander Rush and Stuart Shieber. His research focuses on natural language processing and machine learning, particularly implicit chain-of-thought (CoT) reasoning. In 2023, he introduced the idea of implicit CoT reasoning using hidden states in Implicit Chain-of-Thought Reasoning via Knowledge Distillation, and further advanced this idea in From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Nishanth Dikkala"></td>
    <td style="border: none;"><a href="https://nishanthdikkala.github.io/">Nishanth Dikkala</a><br>Senior Research Scientist at Google. He is interested in efficient architecture design, augmenting LLM reasoning capabilities, and learning theory. He holds a Ph.D. from MIT and a Bachelors from Indian Institute of Technology Bombay.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Jiaheng Liu"></td>
    <td style="border: none;"><a href="https://liujiaheng.github.io/">Jiaheng Liu</a><br>Assistant professor at Nanjing University. His current research mainly focuses on Large Language Models (LLMs), with contributions in pre-training, alignment, and open science of LLMs. He has published 60+ top conference/journal papers, including the ACL 2024 Outstanding Paper Award. Additionally, he has served as the invited speaker of the first GLOW workshop at IJCAI 2023, and the organizer of the first SCI-FM workshop at ICLR 2025.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Zhiyuan Li"></td>
    <td style="border: none;"><a href="https://zhiyuanli.ttic.edu/">Zhiyuan Li</a><br>Assistant professor in the Toyota Technological Institute at Chicago (TTIC) and an affiliated faculty at Uchicago CS. He is currently a visiting faculty at Google Research. He obtained his Ph.D. in computer science at Princeton University in 2022 and spent one year for postdoc at Stanford CS. His research focuses on machine learning theory. He is a recipient of Microsoft Research PhD Fellowship. He is the leading organizer of the first and second workshop for Mathematics in Modern Machine Learning (M3L) at NeurIPS 2023 and 2024.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Wenhao Huang"></td>
    <td style="border: none;"><a href="https://scholar.google.com/citations?user=OdE3MsQAAAAJ&hl=zh-CN">Wenhao Huang</a><br>Researcher at ByteDance. He was a co-founder of 01.AI, responsible for pre-training and multimodal models. He holds a Ph.D. from Peking University. In recent years, he mainly focused on research on large language models and multimodal models, overseeing the training and release of 01.AI's large model Yi-large, as well as open-source models such as Yi-34B, achieving excellent results on platforms like LMSYS and Hugging Face's LLM leaderboard.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Sashank J. Reddi"></td>
    <td style="border: none;"><a href="https://scholar.google.com/citations?user=70lgwYwAAAAJ&hl=en">Sashank J. Reddi</a><br>Senior Staff Research Scientist at Google. He is broadly interested in theory and applications of large ML foundation models and Gen AI, particularly in the intersection of deep learning, optimization, reasoning and efficiency. He won several awards including Neurips best student paper award, ICLR best paper award and ICBS Frontier of Science award. He received a PhD in Computer Science from Carnegie Mellon University and a bachelors in Computer Science from the Indian Institute of Technology Bombay. He was the primary organizer for OPT workshop at NeurIPS for several years.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Chongxuan Li"></td>
    <td style="border: none;"><a href="https://zhenxuan00.github.io/">Chongxuan Li</a><br>Associate professor at Renmin University of China. He holds both a Bachelor's and a Ph.D. degree from Tsinghua University. His research interests include generative models, deep learning, and foundation models. His work was recognized with the Outstanding Paper Award at ICLR 2022. Recently, his group developed LLaDA, a large language diffusion model.</td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/assets/img/organizers/avatar.png" width="200px" alt="Sanjiv Kumar"></td>
    <td style="border: none;"><a href="https://www.sanjivk.com/">Sanjiv Kumar</a><br>Google Fellow and VP at Google Research, where he is leading a team on theory and applications of large ML Foundation Models and Generative AI. His recent research interests include rethinking existing modeling and compute paradigms in LLMs with a focus on developing alternative techniques that allow fast training and inference. Sanjiv has published more than 125 papers in the field of machine learning, computer vision and robotics, and holds 60+ patents. His work on convergence of Adam received the best paper award in ICLR, 2018. He is an action editor of JMLR and holds a PhD (2005) from the School of Computer Science at Carnegie Mellon University.</td>
  </tr>
</table>
